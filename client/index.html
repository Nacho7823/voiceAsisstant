<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Traductor de Voz en Tiempo Real</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            padding: 2em;
            background-color: #f4f7f6;
            display: grid;
            place-items: center;
            min-height: 90vh;
        }
        main {
            width: 100%;
            max-width: 800px;
            background: #ffffff;
            border-radius: 12px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.07);
            padding: 2.5em;
            border: 1px solid #e0e0e0;
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            margin-top: 0;
            margin-bottom: 1.5em;
        }
        h2 {
            font-size: 1.2rem;
            color: #555;
            margin-top: 2em;
            margin-bottom: 0.5em;
            border-bottom: 1px solid #eee;
            padding-bottom: 0.5em;
        }
        .controls {
            display: flex;
            flex-direction: column;
            gap: 1.5em;
            margin-bottom: 2em;
        }
        .buttons {
            display: flex;
            gap: 1em;
        }
        button {
            font-size: 1rem;
            padding: 0.8em 1.5em;
            border-radius: 8px;
            border: none;
            cursor: pointer;
            transition: all 0.2s ease;
            font-weight: 600;
        }
        #start-btn {
            background-color: #27ae60;
            color: white;
            flex-grow: 1;
        }
        #start-btn:hover { background-color: #2ecc71; }
        #start-btn:disabled { background-color: #95a5a6; cursor: not-allowed; }
        
        #stop-btn {
            background-color: #c0392b;
            color: white;
            flex-grow: 1;
        }
        #stop-btn:hover { background-color: #e74c3c; }
        #stop-btn:disabled { background-color: #95a5a6; cursor: not-allowed; }
        
        .selectors {
            display: flex;
            gap: 1em;
            justify-content: space-between;
        }
        .form-group {
            display: flex;
            flex-direction: column;
            gap: 0.5em;
            flex-grow: 1;
        }
        label {
            font-weight: 600;
            color: #555;
            font-size: 0.9rem;
        }
        select, input[type="text"], input[type="password"], textarea {
            font-size: 1rem;
            padding: 0.8em;
            border: 1px solid #ddd;
            border-radius: 8px;
            background-color: #fff;
            width: 100%;
            box-sizing: border-box; /* Para que el padding no rompa el ancho */
        }
        textarea {
            font-family: inherit;
            resize: vertical;
        }
        .status {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 0.8em;
            margin-bottom: 1.5em;
            min-height: 25px;
        }
        .vad-light {
            width: 20px;
            height: 20px;
            background-color: #bdc3c7; /* Gris */
            border-radius: 50%;
            transition: background-color 0.2s;
        }
        .vad-light.speaking {
            background-color: #27ae60; /* Verde */
            animation: pulse 1s infinite;
        }
        #status-log {
            font-size: 1rem;
            color: #34495e;
            font-weight: 500;
        }
        #output-container, #llm-output-container {
            background: #f9f9f9;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            min-height: 100px;
            max-height: 250px;
            overflow-y: auto;
            padding: 1.5em;
            color: #2c3e50;
            line-height: 1.6;
        }
        #llm-output-container {
            background: #f0f4f8; /* Color diferente para LLM */
        }
        #output-container p, #llm-output-container p {
            margin: 0 0 0.8em 0;
            padding-bottom: 0.8em;
            border-bottom: 1px solid #eee;
        }
        #output-container p:last-child, #llm-output-container p:last-child {
            margin-bottom: 0;
            border-bottom: none;
        }
        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(39, 174, 96, 0.7); }
            70% { box-shadow: 0 0 0 10px rgba(39, 174, 96, 0); }
            100% { box-shadow: 0 0 0 0 rgba(39, 174, 96, 0); }
        }
    </style>
</head>
<body>

    <main>
        <h1>Traductor de Voz en Tiempo Real</h1>

        <div class="controls">
            <div class="buttons">
                <button id="start-btn">Iniciar Traducción</button>
                <button id="stop-btn" disabled>Detener</button>
            </div>
            
            <div class="selectors">
                <div class="form-group">
                    <label for="model-select">Modelo Whisper:</label>
                    <select id="model-select">
                        <option value="tiny">Tiny</option>
                        <option value="base" selected>Base</option>
                        <option value="small">Small</option>
                        <option value="medium">Medium</option>
                        <option value="large-v3">Large-v3</option>
                    </select>
                </div>
                <div class="form-group">
                    <label for="language-select">Idioma (Fuente):</label>
                    <select id="language-select">
                        <option value="">Auto-Detectar</option>
                        <option value="es">Español</option>
                        <option value="en">Inglés</option>
                        <option value="fr">Francés</option>
                        <option value="de">Alemán</option>
                        <option value="ja">Japonés</option>
                        <option value="zh">Chino</option>
                        <option value="it">Italiano</option>
                    </select>
                </div>
            </div>

            <!-- SECCIÓN DE LLM MODIFICADA -->
            <div class="form-group">
                <label for="llm-api-url">URL de la API (Endpoint):</label>
                <input type="text" id="llm-api-url" placeholder="Ej: https://generativelanguage.googleapis.com/v1beta/openai/chat/completions">
            </div>
            <div class="selectors">
                <div class="form-group">
                    <label for="llm-api-key">API Key:</label>
                    <input type="password" id="llm-api-key" placeholder="Tu API Key (GEMINI_API_KEY, etc.)">
                </div>
                <div class="form-group">
                    <label for="llm-model-name">Modelo LLM:</label>
                    <input type="text" id="llm-model-name" placeholder="Ej: gemini-2.0-flash">
                </div>
            </div>
            <div class="form-group">
                <label for="llm-prompt">Prompt del LLM (opcional):</label>
                <textarea id="llm-prompt" rows="3" placeholder="Ej: Resume el siguiente texto en una frase:"></textarea>
            </div>
            <!-- FIN DE SECCIÓN MODIFICADA -->

        </div>

        <div class="status">
            <div class="vad-light" id="vad-light"></div>
            <div id="status-log">Haz clic en "Iniciar" para comenzar</div>
        </div>

        <h2>Transcripción (Whisper)</h2>
        <div id="output-container"></div>
        
        <h2>Respuesta (LLM)</h2>
        <div id="llm-output-container"></div>
    </main>

    <script type="module">
        // --- Referencias al DOM ---
        const startBtn = document.getElementById('start-btn');
        const stopBtn = document.getElementById('stop-btn');
        const statusLog = document.getElementById('status-log');
        const vadLight = document.getElementById('vad-light');
        const outputDiv = document.getElementById('output-container');
        const modelSelect = document.getElementById('model-select');
        const languageSelect = document.getElementById('language-select');
        // Nuevos elementos
        const llmApiUrl = document.getElementById('llm-api-url');
        const llmPrompt = document.getElementById('llm-prompt');
        const llmOutputDiv = document.getElementById('llm-output-container');
        // Nuevos elementos para OpenAI
        const llmApiKey = document.getElementById('llm-api-key');
        const llmModelName = document.getElementById('llm-model-name');


        // --- Configuración de APIs y Audio ---
        const VAD_API_URL = "ws://127.0.0.1:8001/ws/vad";
        const WHISPER_API_URL = "http://127.0.0.1:8000/translate";
        
        const SAMPLE_RATE = 16000;
        const PRE_ROLL_TIME = 2; 
        const POST_ROLL_TIME = 1;

        const PRE_ROLL_SAMPLES = SAMPLE_RATE * PRE_ROLL_TIME;
        
        // --- Variables de Estado Globales ---
        let socket = null;
        let audioContext = null;
        let workletNode = null;
        let mediaStreamSource = null;

        let preRollBuffer = new Float32Array(0);
        let speechBuffer = new Float32Array(0);
        let isRecording = false;
        let postRollTimer = null;

        /**
         * Actualiza el log de estado y el estilo de la luz VAD.
         */
        function log(message, state = 'idle') {
            statusLog.textContent = message;
            if (state === 'speaking') {
                vadLight.classList.add('speaking');
            } else {
                vadLight.classList.remove('speaking');
            }
            if (state === 'error') {
                statusLog.style.color = '#c0392b';
            } else {
                statusLog.style.color = '#34495e';
            }
        }

        /**
         * Maneja los datos de audio entrantes desde el AudioWorklet.
         */
        function handleAudioData(audioData) {
            if (isRecording) {
                const newBuffer = new Float32Array(speechBuffer.length + audioData.length);
                newBuffer.set(speechBuffer);
                newBuffer.set(audioData, speechBuffer.length);
                speechBuffer = newBuffer;
            } else {
                const newBuffer = new Float32Array(preRollBuffer.length + audioData.length);
                newBuffer.set(preRollBuffer);
                newBuffer.set(audioData, preRollBuffer.length);
                
                if (newBuffer.length > PRE_ROLL_SAMPLES) {
                    preRollBuffer = newBuffer.slice(newBuffer.length - PRE_ROLL_SAMPLES);
                } else {
                    preRollBuffer = newBuffer;
                }
            }
        }
        
        /**
         * FUNCIÓN MODIFICADA: Envía el texto a una API tipo OpenAI.
         * @param {string} textFromWhisper - El texto devuelto por Whisper.
         */
        async function sendToLLM(textFromWhisper) {
            const apiUrl = llmApiUrl.value.trim();
            const apiKey = llmApiKey.value.trim();
            const modelName = llmModelName.value.trim();
            const userPrompt = llmPrompt.value.trim();
            
            // Si no hay URL, API Key o Modelo, no hacemos nada.
            if (!apiUrl || !apiKey || !modelName) {
                log("Escuchando... (Falta URL, API Key o Modelo de LLM)", "idle");
                return; 
            }

            log("Enviando a LLM...", "processing");

            // Construir el prompt final
            let fullPrompt = textFromWhisper;
            if (userPrompt) {
                fullPrompt = `${userPrompt}\n\n"${textFromWhisper}"`;
            }

            // Payload en formato OpenAI
            const payload = {
                "model": modelName,
                "messages": [
                    {"role": "user", "content": fullPrompt}
                ]
            };

            // Encabezados con Autorización
            const headers = {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`
            };

            try {
                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: headers,
                    body: JSON.stringify(payload)
                });

                const data = await response.json();

                if (!response.ok) {
                    // Si la API devuelve un error (ej: 401, 404, 500), 'data' puede contener el mensaje
                    let errorMsg = data.error?.message || `Error HTTP: ${response.status}`;
                    throw new Error(errorMsg);
                }

                // Parsear respuesta formato OpenAI
                const llmText = data.choices[0].message.content;
                
                if (llmText && llmText.trim()) {
                    llmOutputDiv.innerHTML += `<p>${llmText.trim()}</p>`;
                    llmOutputDiv.scrollTop = llmOutputDiv.scrollHeight;
                }

                log("Escuchando...", "idle"); // Volver a escuchar después de la respuesta del LLM

            } catch (error) {
                console.error('Error al enviar a LLM:', error);
                log(`Error de LLM: ${error.message}`, "error");
                llmOutputDiv.innerHTML += `<p style="color:red;"><strong>Error de LLM:</strong> ${error.message}</p>`;
                llmOutputDiv.scrollTop = llmOutputDiv.scrollHeight;
            }
        }


        /**
         * Envía el audio acumulado a la API de Whisper.
         */
        async function sendToWhisper() {
            if (speechBuffer.length < SAMPLE_RATE * 0.5) { 
                log("Audio demasiado corto, descartado.", "idle");
                speechBuffer = new Float32Array(0); 
                return;
            }
            
            log("Traduciendo (Whisper)...", "processing");
            
            const wavBlob = createWavBlob(speechBuffer, SAMPLE_RATE);
            speechBuffer = new Float32Array(0);
            
            const formData = new FormData();
            formData.append('audio_file', wavBlob, 'segment.wav');
            formData.append('model_size', modelSelect.value);
            formData.append('language', languageSelect.value);

            try {
                const response = await fetch(WHISPER_API_URL, {
                    method: 'POST',
                    body: formData,
                });

                if (!response.ok) {
                    throw new Error(`Error de red: ${response.statusText}`);
                }

                const data = await response.json();
                const translationText = data.result_text; 

                if (typeof translationText === 'string') {
                    const trimmedText = translationText.trim();
                    if (trimmedText) {
                        // 1. Mostrar la transcripción de Whisper
                        outputDiv.innerHTML += `<p>${trimmedText}</p>`;
                        outputDiv.scrollTop = outputDiv.scrollHeight;
                        
                        // 2. Enviar el texto al LLM
                        await sendToLLM(trimmedText);
                        
                    } else {
                        log("Escuchando...", "idle");
                    }
                } else {
                    console.warn("La respuesta de Whisper no contenía una clave 'result_text' válida.");
                    log("Escuchando...", "idle");
                }
                
            } catch (error) {
                console.error('Error al enviar a Whisper:', error);
                log(`Error de Whisper: ${error.message}`, "error");
            }
        }

        /**
         * Función para detener todo (botón de stop o error).
         */
        function stopDetection() {
            if (socket) {
                socket.close();
                socket = null;
            }
            if (workletNode) {
                workletNode.port.onmessage = null;
                workletNode.disconnect();
                workletNode = null;
            }
            if (mediaStreamSource) {
                mediaStreamSource.mediaStream.getTracks().forEach(track => track.stop());
                mediaStreamSource.disconnect();
                mediaStreamSource = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            if (postRollTimer) {
                clearTimeout(postRollTimer);
                postRollTimer = null;
            }
            
            preRollBuffer = new Float32Array(0);
            speechBuffer = new Float32Array(0);
            isRecording = false;

            log('Haz clic en "Iniciar" para comenzar', 'idle');
            startBtn.disabled = false;
            stopBtn.disabled = true;
            // Habilitar todos los controles
            modelSelect.disabled = false;
            languageSelect.disabled = false;
            llmApiUrl.disabled = false;
            llmPrompt.disabled = false;
            llmApiKey.disabled = false;
    llmModelName.disabled = false;
        }

        // --- Event Listener del Botón de Inicio ---
        startBtn.addEventListener('click', async () => {
            log("Iniciando...", "processing");
            startBtn.disabled = true;
            stopBtn.disabled = false;
            // Deshabilitar todos los controles
            modelSelect.disabled = true;
            languageSelect.disabled = true;
            llmApiUrl.disabled = true;
            llmPrompt.disabled = true;
            llmApiKey.disabled = true;
            llmModelName.disabled = true;

            try {
                // 1. Conectar al WebSocket de VAD
                socket = new WebSocket(VAD_API_URL);

                socket.onopen = () => {
                    log("VAD conectado. Solicitando micrófono...", "processing");
                };

                socket.onclose = () => {
                    log("VAD desconectado.", "idle");
                    stopDetection();
                };

                socket.onerror = (err) => {
                    log("Error de WebSocket VAD.", "error");
                    console.error("Error de WebSocket:", err);
                    stopDetection();
                };

                // 2. Escuchar eventos del servidor VAD
                socket.onmessage = (event) => {
                    const msg = JSON.parse(event.data);
                    
                    if (msg.event === "speech_start") {
                        log("Hablando...", "speaking");
                        
                        if (postRollTimer) {
                            clearTimeout(postRollTimer);
                            postRollTimer = null;
                        }
                        if (!isRecording) {
                            speechBuffer = new Float32Array(preRollBuffer);
                        }
                        isRecording = true;

                    } else if (msg.event === "speech_end") {
                        log("Fin de voz detectado...", "processing");
                        
                        if (isRecording) {
                            isRecording = false;
                            postRollTimer = setTimeout(() => {
                                sendToWhisper();
                                postRollTimer = null;
                            }, POST_ROLL_TIME * 1000);
                        }
                    } else if (msg.error) {
                        log(`Error del servidor VAD: ${msg.error}`, "error");
                    }
                };

                // 3. Configurar AudioContext y Worklet
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: SAMPLE_RATE
                });
                
                await audioContext.resume();

                // 4. Crear el Worklet (usando data: URL para evitar errores de 'blob:null')
                const workletBlob = new Blob([`
                    class VADAudioProcessor extends AudioWorkletProcessor {
                        process(inputs, outputs, parameters) {
                            const inputChannel = inputs[0][0];
                            if (inputChannel) {
                                // Enviar una copia, no transferir, para que 
                                // el hilo principal pueda enviarlo al WebSocket
                                this.port.postMessage(inputChannel.slice());
                            }
                            return true;
                        }
                    }
                    registerProcessor('vad-audio-processor', VADAudioProcessor);
                `], { type: 'application/javascript' });
                
                const workletURL = URL.createObjectURL(workletBlob);
                
                await audioContext.audioWorklet.addModule(workletURL);
                URL.revokeObjectURL(workletURL); // Limpiar el blob URL

                workletNode = new AudioWorkletNode(audioContext, 'vad-audio-processor');

                // 5. Configurar el listener del Worklet
                workletNode.port.onmessage = (event) => {
                    // event.data es un Float32Array
                    handleAudioData(event.data);
                    
                    // Enviar audio al servidor VAD
                    if (socket && socket.readyState === WebSocket.OPEN) {
                        socket.send(event.data.buffer);
                    }
                };

                // 6. Obtener micrófono y conectar el grafo de audio
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                if (!audioContext) {
                    throw new Error("El contexto de audio se cerró debido a un error de conexión.");
                }

                mediaStreamSource = audioContext.createMediaStreamSource(stream);
                mediaStreamSource.connect(workletNode);
                workletNode.connect(audioContext.destination);

                log("Micrófono activo. Escuchando...", "idle");

            } catch (error) {
                log(`Error al iniciar: ${error.message}`, "error");
                console.error("Error en startBtn:", error);
                stopDetection();
            }
        });

        stopBtn.addEventListener('click', stopDetection);


        // --- Funciones de Utilidad (Crear WAV) ---
        function createWavBlob(audioData, sampleRate) {
            const buffer = new ArrayBuffer(44 + audioData.length * 2);
            const view = new DataView(buffer);
            
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + audioData.length * 2, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true); // PCM
            view.setUint16(22, 1, true); // 1 canal
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true); // Byte rate
            view.setUint16(32, 2, true); // Block align
            view.setUint16(34, 16, true); // 16-bit
            writeString(view, 36, 'data');
            view.setUint32(40, audioData.length * 2, true);
            
            let offset = 44;
            for (let i = 0; i < audioData.length; i++, offset += 2) {
                const s = Math.max(-1, Math.min(1, audioData[i]));
                view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }
            
            return new Blob([view], { type: 'audio/wav' });
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

    </script>
</body>
</html>

